import functools
import math
import random
from typing import Callable, Literal, NewType, Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, reduce

from text_sed.layers import PretrainedEmbedding, PretrainedUnEmbedding

from . import utils

Device = NewType("Device", torch.device)
DType = NewType("DType", torch.dtype)
Shape = NewType("Shape", Tuple[int, ...])
Tensor = NewType("Tensor", torch.Tensor)
NamedTensor = Literal  # **Naive** named tensor
Generator = NewType("Generator", torch.Generator)


# Noise Schedules
# Reference: https://github.com/openai/improved-diffusion/blob/main/improved_diffusion/gaussian_diffusion.py
# @ unixpickle ğŸ¤˜ğŸ¥’


def get_noise_schedule(schedule_name: str, **kwargs) -> Callable:
    """Returns Î±Ì„ schedules"""
    if schedule_name == "cosine":
        return cosine_alpha_bar_schedule(**kwargs)
    elif schedule_name == "linear":
        return linear_schedule(**kwargs)
    raise ValueError(f"Schedule `{schedule_name}` is not available.")


def linear_schedule(start: float, end: float) -> Tensor:
    """Linear noise-variance (Î²) schedule."""

    def scheduler(num_steps: int):
        return torch.linspace(start, end, num_steps)

    return scheduler


def cosine_alpha_bar(
    time: float,
    offset: Optional[float] = 0.0002,
) -> Tensor:
    """Cosine noise-variance Î±Ì„ scheduler (Î±Ì„[t] = Î áµ—Î±[i] where Î±[i] = (1 - Î²[i]))
    for continuous time parameterization.

    Reference: Nichol & Dhariwal, "Improved Denoising Diffusion Probabilistic Models".
        2021. https://arxiv.org/pdf/2102.09672.pdf

    Args:
        offset: Small offset to prevent Î²â‚œ from beeing too small near
            t = 0.
    """
    return torch.cos(((time + offset) / (1 + offset)) * torch.pi / 2) ** 2


def cosine_alpha_bar_schedule(
    offset: Optional[float] = 0.0002,
) -> Tensor:
    """Cosine noise-variance (Î²) scheduler

    Reference: Nichol & Dhariwal, "Improved Denoising Diffusion Probabilistic Models".
        2021. https://arxiv.org/pdf/2102.09672.pdf

    Args:
        offset: Small offset to prevent Î²â‚œ from beeing too small near
            t = 0.
    """

    def scheduler(num_steps: float):
        return cosine_alpha_bar(time=num_steps, offset=offset)

    return scheduler


# Samplers


def get_sampler(sampler_name: str, **kwargs) -> Callable:
    """Returns sampler step function"""
    if sampler_name == "ddim":
        return functools.partial(ddim_step, **kwargs)
    elif sampler_name == "ddpm":
        return functools.partial(ddpm_step, **kwargs)
    else:
        raise ValueError(f"Sampler `{sampler_name}` is not available.")


def ddim_step(
    noisy_inputs: Tensor,  # xâ‚œ
    pred_inputs: Tensor,  # xÌƒâ‚€
    time_now: Tensor,  # t
    time_next: Tensor,  # t - 1
    schedule: Callable,
    scale: float = 1.0,
) -> Tensor:  # xâ‚œâ‚‹â‚
    """Denoising diffusion implicit model step with Î· = 0. Estimates xâ‚€ at
    time_next with the DDIM updating rule.

    References:
    - Song et al. "Denoising Diffusion Implicit Models". 2020.
        https://arxiv.org/pdf/2010.02502.pdf
    - Lilian Weng.
        https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#speed-up-diffusion-model-sampling
    - Clamping Trick (see footnote 6 in the paper):
        Li et al. "Diffusion-LM Improves Controllable Text Generation". 2022
    """
    # TODO: Remove unicode characters after coming up with good var names.
    xâ‚œ, xÌƒâ‚’ = noisy_inputs, pred_inputs
    xÌƒâ‚’ = xÌƒâ‚’.clamp(-scale, scale)
    Î±Ì„â‚œ, Î±Ì„â‚™ = schedule(time_now), schedule(time_next)  # Î±Ì„â‚™ = Î±Ì„â‚œâ‚‹â‚
    Ïµ = (xâ‚œ - torch.sqrt(Î±Ì„â‚œ) * xÌƒâ‚’) * torch.rsqrt(1 - Î±Ì„â‚œ)
    # Next estimate (xâ‚™ := xâ‚œâ‚‹â‚)
    xâ‚™ = torch.sqrt(Î±Ì„â‚™) * xÌƒâ‚’ + torch.sqrt(1 - Î±Ì„â‚™) * Ïµ
    return xâ‚™


def ddpm_step(
    noisy_inputs: Tensor,  # xâ‚œ
    pred_inputs: Tensor,   # xÌƒâ‚€
    time_now: Tensor,      # t
    time_next: Tensor,     # t - 1
    schedule: Callable,
    scale: float = 1.0,
) -> Tensor:               # xâ‚œâ‚‹â‚
    """Denoising diffusion implicit model step with Î· = 1. Estimates xâ‚€ at
    time_next with the DDPM updating rule.

    References:
    - Ho et al. "Denoising Diffusion Probabilistic Models". 2020.
        https://arxiv.org/abs/2006.11239
    - Clamping Trick (see footnote 6 in the paper):
        Li et al. "Diffusion-LM Improves Controllable Text Generation". 2022
    """
    xâ‚œ, xÌƒâ‚’ = noisy_inputs, pred_inputs
    xÌƒâ‚’ = xÌƒâ‚’.clamp(-scale, scale)
    Î³â‚œ = schedule(time_now)
    Î±Ì„â‚œ = Î³â‚œ / schedule(time_next)
    Ïƒâ‚œ = torch.sqrt(1 - Î±Ì„â‚œ)
    z = torch.randn_like(Ïƒâ‚œ)
    Ïµ = (xâ‚œ - torch.sqrt(Î³â‚œ) * xÌƒâ‚’) * torch.rsqrt(1 - Î³â‚œ)
    xâ‚™ = torch.rsqrt(Î±Ì„â‚œ) * (xâ‚œ - ((1 - Î±Ì„â‚œ) * torch.rsqrt(1 - Î³â‚œ)) * Ïµ) + Ïƒâ‚œ * z
    return xâ‚™


# Diffusion


def corrupt(
    inputs: Tensor,  # xâ‚€
    time: Tensor,    # t
    schedule: Callable,  # Î±Ì„ schedule
) -> Tensor:
    """q sampler: q(xâ‚œ | xâ‚’) ~ N(xâ‚’ * âˆšÎ±Ì„â‚œ, (1 - Î±Ì„â‚œ)I)
    Arbitrary time q-sampler for forward diffusion processing (corruption).

    Reference
    - Ho et al. "Denoising Diffusion Probabilistic Models". 2020.
        https://arxiv.org/abs/2006.11239
    """
    noise = torch.randn_like(inputs)  # Ïµ

    signal_rate = torch.sqrt(schedule(time))     # âˆšÎ±Ì„â‚œ
    noise_rate = torch.sqrt(1 - schedule(time))  # âˆš(1 - Î±Ì„â‚œ)

    signal_rate = utils.append_dims(signal_rate, inputs.ndim)
    noise_rate = utils.append_dims(noise_rate, inputs.ndim)
    return signal_rate * inputs + noise_rate * noise


def cross_entropy_loss(
    logits: Tensor,
    targets: Tensor,
    z_loss: Optional[float] = 0.0,
) -> float:
    """Mesh-transformer-jax style cross entropy loss.
    Args:
        logits: The unnormalized label scores.
        targets: The ground truth labels. These should be one-hot encoded.
    """
    logits -= torch.max(logits, dim=-1, keepdim=True)[0]
    one_hot_targets = F.one_hot(targets, num_classes=logits.shape[-1])
    predicted_logits = torch.sum(one_hot_targets * logits, dim=-1)
    loss = -predicted_logits + torch.logsumexp(logits, dim=-1)
    # Add the auxiliary z-loss term
    loss += torch.mean(z_loss * (1e-4 * torch.square(torch.logsumexp(logits, dim=-1))))
    loss = reduce(loss, "b ... -> 1", "mean")[0]
    # Compute the fraction of correct predictions per batch:
    correct = (torch.argmax(logits, dim=-1) == targets).float()
    correct = reduce(correct, "b ... -> 1", "mean")[0]
    return dict(loss=loss, correct=correct)


class TextSed(nn.Module):
    def __init__(
        self,
        model: nn.Module,
        embed_mat: NamedTensor["vocab", "embed"],
        use_self_cond: bool = True,
        noise_schedule: Callable = get_noise_schedule("cosine"),
        bottleneck_dim: Optional[int] = None,
    ):
        super().__init__()
        self.model = model
        self.use_self_cond = use_self_cond
        self.noise_schedule = noise_schedule

        _, embed_dim = embed_mat.shape

        # Discrete-to-continuous fixed read-in matrix: E Ïµ Râ±½Ë£á´°
        self.read_in = nn.Sequential(
            PretrainedEmbedding(embed_mat, use_normalization=True),
            *[
                # Bottleneck layer to shrink word embeddings: D â†’ D'
                nn.LayerNorm(embed_dim),
                nn.Linear(embed_dim, bottleneck_dim)
            ] if bottleneck_dim else [nn.Identity()],
        )
        # Continous-to-discrete learnable read-out matrix
        self.read_out = nn.Sequential(
            *[
                # "Add a linear output projection layer Eâ€² which takes the output of
                # the transformer y âˆˆ Rá´ºË£á´° and projects each element (yáµ¢) 1 â‰¤ i â‰¤ N
                # back to the same size as the word embeddings, `embed_dim`."
                nn.Linear(bottleneck_dim, embed_dim),
                nn.LayerNorm(embed_dim),
            ] if bottleneck_dim else [nn.Identity()],
            # Initalize read-out (R) to: Eáµ€ Ïµ Rá´°Ë£â±½
            PretrainedUnEmbedding(embed_mat, use_renormalization=False),
         ) # Eâ€²

    def forward(
        self,
        inputs: NamedTensor["batch", "pos", "embed"],
        use_self_cond: Optional[bool] = True,
        z_loss: Optional[float] = 0.0,
    ) -> NamedTensor["batch", "pos", "embed"]:
        """
        Args:
            - inputs: The input token sequence.
        """
        batch_size = inputs.shape[0]

        # Discrete-to-continuous data embedding (token/word -> embedding)
        embeds = self.read_in(inputs)

        # Select random timesteps and corrupt
        time = torch.rand((batch_size,), device=embeds.device)
        noisy_embeds = corrupt(embeds, time, schedule=self.noise_schedule)

        # Compute self-conditioning estimate
        cond_embeds = torch.zeros_like(noisy_embeds, dtype=noisy_embeds.dtype)
        if use_self_cond and random.random() > 0.5:
            with torch.no_grad():
                cond_embeds = self.model(noisy_embeds, cond_embeds, time).detach()

        # Predict embeddings
        pred_embeds = self.model(noisy_embeds, self_cond=cond_embeds, time=time)
        logits = self.read_out(pred_embeds)

        # Diffusion and Reconstruction loss
        loss_mse = F.mse_loss(pred_embeds, target=embeds, reduction="mean")
        loss_recon = cross_entropy_loss(logits, targets=inputs, z_loss=z_loss)
        total_loss = loss_mse + loss_recon["loss"]

        return total_loss, utils.flatten_dict(
            dict(
                total_loss=total_loss.item(),
                loss_mse=loss_mse.item(),
                loss_recon=loss_recon["loss"].item(),
                correct=loss_recon["correct"],
            )
        )

    @torch.no_grad()
    def sample(
        self,
        shape: Shape,
        num_steps: int,
        *,
        sampler: Optional[Callable] = ddim_step,
        # conds: Optional[NamedTensor["batch", "pos", "embed"]] = None,
        # guide_scale: Optional[float] = None,
        time_delta: Optional[float] = 0.0,
        device: Optional[Device] = "cuda:0",
    ) -> NamedTensor:
        """p sampler
        Sampler for the reverse diffusion process (denoising).

        TODO: Remove unicode characters from code after coming up with concise
        names.

        Args:
            time_delta: Asymmetric time interval shift, t â†’ (t - Î”)
        """
        # Sample start embedding from the normal prior eâ‚œ ~ qâ‚œ
        eâ‚œ = torch.randn(shape, device=device)
        eÌƒâ‚’ = torch.zeros_like(eâ‚œ)

        for step in range(num_steps):
            # Get time for current and next states
            # (NOTE: (1 - ...) to process in reverse)
            time_now = torch.tensor([1 - step / num_steps], device=device)
            time_next = torch.tensor([
                torch.maximum(
                    torch.tensor(1 - (step + 1 + time_delta) / num_steps),
                    torch.tensor(0.0),
                )],
                device=device,
            )

            # if guide_scale is not None and conds is None:  # Self-conditioning guidance
            #     # Predict start embeds (eâ‚’) without self-cond
            #     uÌƒâ‚’ = self.model(eâ‚œ, torch.zeros_like(eâ‚œ), time_now)
            #     # Predict start embeds (eâ‚’) with self-conditiong
            #     cÌƒâ‚’ = self.model(eâ‚œ, uÌƒâ‚’, time_now)
            #     # Apply self-conditioning guidance
            #     eÌƒâ‚’ = guide_scale * cÌƒâ‚’ + (1.0 - guide_scale) * uÌƒâ‚’ 
            # elif guide_scale is not None and conds is not None:  # Classifier Free Guidance
            #     # Predict start embeds (eâ‚’) without self-cond
            #     cond_embeds = self.read_in(conds)
            #     uÌƒâ‚’ = self.model(eâ‚œ, torch.zeros_like(eâ‚œ), time_now)
            #     # Predict start embeds (eâ‚’) with self-conditiong
            #     cÌƒâ‚’ = self.model(eâ‚œ, uÌƒâ‚’, time_now)
            #     # eÌƒâ‚’ = guide_scale * uÌƒâ‚’
            # else:
            # Self-conditioned prediction using the previous predictions, eÌƒâ‚’
            eÌƒâ‚’ = self.model(eâ‚œ, eÌƒâ‚’, time_now)

            # Estimate embeds at time_next eâ‚œâ‚‹â‚
            eâ‚œ = sampler(eâ‚œ, eÌƒâ‚’, time_now, time_next, self.noise_schedule)

        # Token decoding: continous embeddings to discrete tokens
        logits = self.read_out(eÌƒâ‚’)
        tokens = torch.argmax(logits, -1)
        return tokens
